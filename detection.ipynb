{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "  copyrightpath = \"/home/polymath/Desktop/GSOC2021/copyrightByMichael.csv\"\n",
    "  df = pd.read_csv(copyrightpath, error_bad_lines= False, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "  # data.dropna(inplace=True)\n",
    "  data = df.iloc[:10000,:]\n",
    "  data['Notice, this list of conditions, the following disclaimer, and the original OpenSSL and SSLeay Licences below.\",f'] = data['Notice, this list of conditions, the following disclaimer, and the original OpenSSL and SSLeay Licences below.\",f'].astype(str)\n",
    "  data['Notice, this list of conditions, the following disclaimer, and the original OpenSSL and SSLeay Licences below.\",f'] = data['Notice, this list of conditions, the following disclaimer, and the original OpenSSL and SSLeay Licences below.\",f'].str.lower()\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityCheck(listA, listB, index, hit_index):\n",
    "    THE_PROBABLE_LOGIC_POS_CHECK = ['NOUN', 'NUM', 'PROPN', 'PROPN']\n",
    "    THE_PROBABLE_LOGIC_NER_CHECK = ['DATE', 'PERSON', 'CARDINAL', 'ORG']\n",
    "    if set(THE_PROBABLE_LOGIC_POS_CHECK).intersection(set(listA[\"POS_TAG\"])):\n",
    "        if set(THE_PROBABLE_LOGIC_NER_CHECK).intersection(set(listB[\"Values\"])):\n",
    "                for _values in listB[\"Entity\"]:\n",
    "                    try:\n",
    "                        date_object = datetime.datetime.strptime(_values, '%Y').year\n",
    "                        if date_object:\n",
    "                            print(\"It is a hittttt\")\n",
    "                            hit_index.append(index)\n",
    "                            break\n",
    "                    except (TypeError, ValueError) as e:\n",
    "                        pass\n",
    "                    # if THE_PROBABLE_LOGIC_NER_CHECK[0] in _values:\n",
    "                    #     print(\"It is a hit!!!\")\n",
    "                    #     try:\n",
    "                    #         hit_index.append(index)\n",
    "                    #     except:\n",
    "                    #         print(index)\n",
    "        else:\n",
    "            print(\"It is a miss!!!\")\n",
    "    return hit_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "final_indexes = []\n",
    "temp_var = []\n",
    "hit_index = []\n",
    "for index, row in data.iterrows():\n",
    "    text = data.loc[index, 'Notice, this list of conditions, the following disclaimer, and the original OpenSSL and SSLeay Licences below.\",f']\n",
    "    text_split = text.split(\",\")\n",
    "    data.loc[index,'new_tag'] = str(text_split[-1])\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    if type(text) == float:\n",
    "        continue\n",
    "    \n",
    "    ## Lemmatization\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "\n",
    "    \n",
    "    # Filter the stopword\n",
    "    filtered_sentence =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word)\n",
    "\n",
    "    \n",
    "    # Remove punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "\n",
    "    ## List joining\n",
    "    list_of_copyrights = \" \".join(map(str,filtered_sentence))\n",
    "    substring = \"( c )\"\n",
    "    cp_symbol = '\\xa9'\n",
    "    \n",
    "    if \"copyright\" not in list_of_copyrights:\n",
    "        if substring in list_of_copyrights:\n",
    "            list_of_copyrights = list_of_copyrights.replace(substring, \"copyright\")\n",
    "        \n",
    "        elif cp_symbol in list_of_copyrights:\n",
    "            list_of_copyrights = list_of_copyrights.replace(cp_symbol, \"copyright\")\n",
    "   \n",
    "    ## Implementing NER and POS Tags after normalization\n",
    "    doc2 = nlp(list_of_copyrights)\n",
    "\n",
    "    ent_dict = {}\n",
    "\n",
    "    full_table_ner = { \"Entity\": [], \"Values\": []}\n",
    "\n",
    "    for x in doc2.ents:\n",
    "        ent_dict[x.text] = x.label_\n",
    "\n",
    "    for key in ent_dict:\n",
    "        full_table_ner[\"Entity\"].append(key)\n",
    "        full_table_ner[\"Values\"].append(ent_dict[key])\n",
    "\n",
    "    pos_dict = {}\n",
    "    full_table_pos = { \"Entity\": [], \"POS_TAG\": []}\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_punct | token.is_space:\n",
    "            pos_dict[token.text] = token.pos_\n",
    "\n",
    "    for key in pos_dict:\n",
    "        full_table_pos[\"Entity\"].append(key)\n",
    "        full_table_pos[\"POS_TAG\"].append(pos_dict[key])\n",
    "\n",
    "    entityCheck(full_table_pos, full_table_ner, index, hit_index)\n",
    "\n",
    "\n",
    "print(hit_index)\n",
    "expected_time = (time.time() - start)\n",
    "print(expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hit_index:\n",
    "    data.loc[i,'Hit&Miss'] = \"t\"\n",
    "\n",
    "data[\"Hit&Miss\"].fillna(\"f\", inplace=True)\n",
    "data.to_csv('splitted_copyrightsAlgorifyNewtag.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "total_counter = 0\n",
    "for index, row in data.iterrows():\n",
    "    original_tag = data.loc[index, 'new_tag']\n",
    "    algorithm_tag = data.loc[index,'Hit&Miss']\n",
    "\n",
    "    if original_tag == \"t\":\n",
    "        total_counter += 1\n",
    "        if algorithm_tag == original_tag:\n",
    "            counter += 1\n",
    "    \n",
    "accuracy_score = (counter/total_counter) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false = data[\"Hit&Miss\"] == \"t\"\n",
    "all_false_df = data[all_false]\n",
    "all_false_df.to_csv('splitted_copyrightsAlgorifysubDF_TRUEE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true = data[\"Hit&Miss\"] == \"f\"\n",
    "all_true_df = data[all_true]\n",
    "all_true_df.to_csv('splitted_copyrightsAlgorifysubDF_FALSE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "list_date = [\"copyright\", \"1998\", \"1999\", \"1997\", \"1\", \"hfjhf\"]\n",
    "\n",
    "for val in list_date:\n",
    "    try:\n",
    "        b = datetime.datetime.strptime(val, '%Y').year\n",
    "        if b:\n",
    "            print(b)\n",
    "    except (TypeError, ValueError) as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}