{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "copyrightpath = os.getcwd() + \"/YourCSVFile.csv\"  ##Assuming that the File is hard-codedly provided\n",
    "df = pd.read_csv(copyrightpath, error_bad_lines= False, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "data = df.iloc[:10000,:] ##Configured according to Big CSV file\n",
    "data['copyright'] = data['Notice, this list of conditions, the following disclaimer, and the original OpenSSL and SSLeay Licences below.\",f'].astype(str)\n",
    "data['copyright'] = data['copyright'].str.lower()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityCheck(listA, listB, index, hit_index):\n",
    "    THE_PROBABLE_LOGIC_POS_CHECK = ['NOUN', 'NUM', 'PROPN', 'PROPN']\n",
    "    THE_PROBABLE_LOGIC_NER_CHECK = ['DATE', 'PERSON', 'CARDINAL', 'ORG']\n",
    "    if set(THE_PROBABLE_LOGIC_POS_CHECK).intersection(set(listA[\"POS_TAG\"])):\n",
    "        if set(THE_PROBABLE_LOGIC_NER_CHECK).intersection(set(listB[\"Values\"])):\n",
    "                for _values in listB[\"Values\"]:\n",
    "                    if THE_PROBABLE_LOGIC_NER_CHECK[0] in _values:\n",
    "                        print(\"It is a copyright!!\")\n",
    "                        try:\n",
    "                            hit_index.append(index)\n",
    "                        except:\n",
    "                            print(index)\n",
    "                    elif THE_PROBABLE_LOGIC_NER_CHECK[2] in _values:\n",
    "                        for _val in listB[\"Entity\"]:\n",
    "                            try:\n",
    "                                pattern_regex = r'''^((19|20)\\d{2}|\\d{2})([,\\s-]{1,3}((19|20)\\d{2}|\\d{1,2}))*$'''\n",
    "                                extract_list = re.search(pattern_regex,_val)\n",
    "                                if extract_list:\n",
    "                                    print(\"It is a copyright!!\")\n",
    "                                    hit_index.append(index)\n",
    "                            except:\n",
    "                                print(index)\n",
    "\n",
    "        else:\n",
    "            print(\"It is a not a copyright!!\")\n",
    "    return hit_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyScore():\n",
    "    counter = 0\n",
    "    total_counter = 0\n",
    "    for index, row in data.iterrows():\n",
    "        original_tag = data.loc[index, 'new_tag']\n",
    "        algorithm_tag = data.loc[index,'Hit&Miss']\n",
    "\n",
    "        if original_tag == \"t\":\n",
    "            total_counter += 1\n",
    "            if algorithm_tag == original_tag:\n",
    "                counter += 1\n",
    "\n",
    "    accuracy_score = str((counter/total_counter) * 100) + \" %\"\n",
    "    \n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing():\n",
    "    start = time.time()\n",
    "    final_indexes = []\n",
    "    temp_var = []\n",
    "    hit_index = []\n",
    "\n",
    "    ## Iterating through each row and doing preprocessing over it.\n",
    "    ## Picking out the manual tags from the csv and putting them into seperate column \"Original Tag\"\n",
    "    for index, row in data.iterrows():\n",
    "        text = data.loc[index, 'copyright']\n",
    "        text_split = text.split(\",\")\n",
    "        data.loc[index,'new_tag'] = str(text_split[-1])\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        if type(text) == float:\n",
    "            continue\n",
    "        \n",
    "        ## Lemmatization\n",
    "        lemma_list = []\n",
    "        for token in doc:\n",
    "            lemma_list.append(token.lemma_)\n",
    "        \n",
    "        # Filter the stopword\n",
    "        filtered_sentence =[] \n",
    "        for word in lemma_list:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            if lexeme.is_stop == False:\n",
    "                filtered_sentence.append(word)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        punctuations=\"?:!.,;\"\n",
    "        for word in filtered_sentence:\n",
    "            if word in punctuations:\n",
    "                filtered_sentence.remove(word)\n",
    "\n",
    "        ## List joining and Filtering (c) and copyright unicode symbol\n",
    "        list_of_copyrights = \" \".join(map(str,filtered_sentence))\n",
    "        substring = \"( c )\"\n",
    "        cp_symbol = '\\xa9' ##Unicode for copyright Symbol\n",
    "        \n",
    "        if \"copyright\" not in list_of_copyrights:\n",
    "            if substring in list_of_copyrights:\n",
    "                list_of_copyrights = list_of_copyrights.replace(substring, \"copyright\")\n",
    "            \n",
    "            elif cp_symbol in list_of_copyrights:\n",
    "                list_of_copyrights = list_of_copyrights.replace(cp_symbol, \"copyright\")\n",
    "    \n",
    "        ## Implementing NER and POS Tags after normalization\n",
    "        doc2 = nlp(list_of_copyrights)\n",
    "\n",
    "        ## All the NER taggings will be contained in a dictionary having \"Entity\" and \"Values\" as keys\n",
    "        ent_dict = {}\n",
    "\n",
    "        full_table_ner = { \"Entity\": [], \"Values\": []}\n",
    "\n",
    "        for x in doc2.ents:\n",
    "            ent_dict[x.text] = x.label_\n",
    "\n",
    "        for key in ent_dict:\n",
    "            full_table_ner[\"Entity\"].append(key)\n",
    "            full_table_ner[\"Values\"].append(ent_dict[key])\n",
    "        \n",
    "        ## All the POS taggings will be contained in a dictionary having \"Entity\" and \"POS_TAGS\" as keys\n",
    "        pos_dict = {}\n",
    "        full_table_pos = { \"Entity\": [], \"POS_TAG\": []}\n",
    "\n",
    "        for token in doc:\n",
    "            if not token.is_punct | token.is_space:\n",
    "                pos_dict[token.text] = token.pos_\n",
    "\n",
    "        for key in pos_dict:\n",
    "            full_table_pos[\"Entity\"].append(key)\n",
    "            full_table_pos[\"POS_TAG\"].append(pos_dict[key])\n",
    "\n",
    "        ## The checking function call happening with each iteration\n",
    "        entityCheck(full_table_pos, full_table_ner, index, hit_index)\n",
    "\n",
    "    ## Updating the predicted TP results into one different column called \"Hit&Miss\"\n",
    "    for i in hit_index:\n",
    "        data.loc[i,'Hit&Miss'] = \"t\"\n",
    "\n",
    "    data[\"Hit&Miss\"].fillna(\"f\", inplace=True)\n",
    "\n",
    "    percentScore = accuracyScore()\n",
    "    print(percentScore)\n",
    "    expected_time = str((time.time() - start))\n",
    "    print(expected_time + \" sec\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "preProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "87.28056004308023 %\n"
     ]
    }
   ],
   "source": [
    "print(accuracyScore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}