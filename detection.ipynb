{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "copyrightpath = os.getcwd() + \"/YourCSVExample.csv\"  ##Assuming that the File is hard-codedly provided\n",
    "df= pd.read_csv(copyrightpath, error_bad_lines= False, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "df = df.iloc[:100000,:]\n",
    "df['copyright'] = df['Notice, this list of conditions, the following disclaimer, and the original OpenSSL and SSLeay Licences below.\",f']\n",
    "df['copyright'] = df['copyright'].str.lower()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def entityCheck(listA, listB, index, hit_index, clutter_flag):\n",
    "    THE_PROBABLE_LOGIC_POS_CHECK = ['NOUN', 'NUM', 'PROPN', 'PROPN']\n",
    "    THE_PROBABLE_LOGIC_NER_CHECK = ['DATE', 'PERSON', 'CARDINAL', 'ORG']\n",
    "    if set(THE_PROBABLE_LOGIC_POS_CHECK).intersection(set(listA[\"POS_TAG\"])):\n",
    "        if set(THE_PROBABLE_LOGIC_NER_CHECK).intersection(set(listB[\"Values\"])):\n",
    "                for _values in listB[\"Values\"]:\n",
    "                    if THE_PROBABLE_LOGIC_NER_CHECK[0] in _values:\n",
    "                        print(\"It is a copyright!!\")\n",
    "                        hit_index.append(index)\n",
    "                        if clutter_flag:\n",
    "                            clutterRemoval(index, listB)\n",
    "                    elif THE_PROBABLE_LOGIC_NER_CHECK[2] in _values:\n",
    "                        for _val in listB[\"Entity\"]:\n",
    "                                pattern_regex = r'''((?:19|20)\\d{2}|\\d{2})(?!\\d)(?:[, \\t-]{1,3}((?:19|20)\\d{2}|\\d{1,2}))?'''\n",
    "                                extract_list = re.search(pattern_regex,_val)\n",
    "                                if extract_list:\n",
    "                                    print(\"It is a copyright!!\")\n",
    "                                    hit_index.append(index)\n",
    "                                if clutter_flag:\n",
    "                                    clutterRemoval(index, listB)\n",
    "                    elif THE_PROBABLE_LOGIC_NER_CHECK[1] in _values or THE_PROBABLE_LOGIC_NER_CHECK[3] in _values:\n",
    "                        print(\"It is a copyright!!\")\n",
    "                        hit_index.append(index)\n",
    "                        if clutter_flag:\n",
    "                            clutterRemoval(index, listB)\n",
    "        else:\n",
    "            print(\"It is a not a copyright!!\")\n",
    "    return hit_index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def accuracyScore_TP():\n",
    "    counter = 0\n",
    "    total_counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        original_tag = df.loc[index, 'new_tag']\n",
    "        algorithm_tag = df.loc[index,'Hit&Miss']\n",
    "\n",
    "        if original_tag == \"t\":\n",
    "            total_counter += 1\n",
    "            if algorithm_tag == original_tag:\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "    accuracy_score_tp_precision = counter/total_counter\n",
    "    accuracy_score_tp = str((accuracy_score_tp_precision) * 100) + \" %\"\n",
    "    \n",
    "    return accuracy_score_tp_precision, accuracy_score_tp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def accuracyScore_FN():\n",
    "    counter = 0\n",
    "    total_counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        original_tag = df.loc[index, 'new_tag']\n",
    "        algorithm_tag = df.loc[index,'Hit&Miss']\n",
    "\n",
    "        if original_tag == \"f\":\n",
    "            total_counter += 1\n",
    "            if algorithm_tag == \"t\":\n",
    "                counter += 1\n",
    "    accuracy_score_fn_precision = counter/total_counter\n",
    "    accuracy_score_fn = str((accuracy_score_fn_precision) * 100) + \" %\"\n",
    "    \n",
    "    return accuracy_score_fn_precision, accuracy_score_fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def accuracyScore_TN():\n",
    "    counter = 0\n",
    "    total_counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        original_tag = df.loc[index, 'new_tag']\n",
    "        algorithm_tag = df.loc[index,'Hit&Miss']\n",
    "\n",
    "        if original_tag == \"f\":\n",
    "            total_counter += 1\n",
    "            if algorithm_tag == \"f\":\n",
    "                counter += 1\n",
    "    accuracy_score_tn_precision = counter/total_counter\n",
    "    accuracy_score_tn = str((accuracy_score_tn_precision) * 100) + \" %\"\n",
    "    \n",
    "    return accuracy_score_tn_precision, accuracy_score_tn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def accuracyScore_FP():\n",
    "    counter = 0\n",
    "    total_counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        original_tag = df.loc[index, 'new_tag']\n",
    "        algorithm_tag = df.loc[index,'Hit&Miss']\n",
    "\n",
    "        if original_tag == \"t\":\n",
    "            total_counter += 1\n",
    "            if algorithm_tag == \"f\":\n",
    "                counter += 1\n",
    "    accuracy_score_fp_precision = counter/total_counter\n",
    "    accuracy_score_fp = str((accuracy_score_fp_precision) * 100) + \" %\"\n",
    "    \n",
    "    return accuracy_score_fp_precision, accuracy_score_fp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preProcessing():\n",
    "    start = time.time()\n",
    "    hit_index = []\n",
    "    clutter_flag = 0\n",
    "\n",
    "    ## Iterating through each row and doing preprocessing over it.\n",
    "    ## Picking out the manual tags from the csv and putting them into seperate column \"Original Tag\"\n",
    "    for index, row in df.iterrows():\n",
    "        text = df.loc[index, 'copyright']\n",
    "        text_split = text.split(\",\")\n",
    "        df.loc[index,'new_tag'] = str(text_split[-1])\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        if type(text) == float:\n",
    "            continue\n",
    "        \n",
    "        ## Lemmatization\n",
    "        lemma_list = []\n",
    "        for token in doc:\n",
    "            lemma_list.append(token.lemma_)\n",
    "        \n",
    "        # Filter the stopword\n",
    "        filtered_sentence =[] \n",
    "        for word in lemma_list:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            if lexeme.is_stop == False:\n",
    "                filtered_sentence.append(word)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        punctuations=\"?:!.,;\"\n",
    "        for word in filtered_sentence:\n",
    "            if word in punctuations:\n",
    "                filtered_sentence.remove(word)\n",
    "\n",
    "        ## List joining and Filtering (c) and copyright unicode symbol\n",
    "        list_of_copyrights = \" \".join(map(str,filtered_sentence))\n",
    "        substring = \"( c )\"\n",
    "        cp_symbol = '\\xa9' ##Unicode for copyright Symbol\n",
    "        \n",
    "        if \"copyright\" not in list_of_copyrights:\n",
    "            if substring in list_of_copyrights:\n",
    "                list_of_copyrights = list_of_copyrights.replace(substring, \"copyright\")\n",
    "            \n",
    "            elif cp_symbol in list_of_copyrights:\n",
    "                list_of_copyrights = list_of_copyrights.replace(cp_symbol, \"copyright\")\n",
    "                \n",
    "        if substring in list_of_copyrights:\n",
    "            list_of_copyrights = list_of_copyrights.replace(substring, \"copyright\")\n",
    "\n",
    "        elif cp_symbol in list_of_copyrights:\n",
    "            list_of_copyrights = list_of_copyrights.replace(cp_symbol, \"copyright\")\n",
    "    \n",
    "        ## Implementing NER and POS Tags after normalization\n",
    "        doc2 = nlp(list_of_copyrights)\n",
    "\n",
    "        ## All the NER taggings will be contained in a dictionary having \"Entity\" and \"Values\" as keys\n",
    "        ent_dict = {}\n",
    "\n",
    "        full_table_ner = { \"Entity\": [], \"Values\": []}\n",
    "\n",
    "        for x in doc2.ents:\n",
    "            ent_dict[x.text] = x.label_\n",
    "\n",
    "        for key in ent_dict:\n",
    "            full_table_ner[\"Entity\"].append(key)\n",
    "            full_table_ner[\"Values\"].append(ent_dict[key])\n",
    "        \n",
    "        ## All the POS taggings will be contained in a dictionary having \"Entity\" and \"POS_TAGS\" as keys\n",
    "        pos_dict = {}\n",
    "        full_table_pos = { \"Entity\": [], \"POS_TAG\": []}\n",
    "\n",
    "        for token in doc:\n",
    "            if not token.is_punct | token.is_space:\n",
    "                pos_dict[token.text] = token.pos_\n",
    "\n",
    "        for key in pos_dict:\n",
    "            full_table_pos[\"Entity\"].append(key)\n",
    "            full_table_pos[\"POS_TAG\"].append(pos_dict[key])\n",
    "\n",
    "        ## The checking function call happening with each iteration\n",
    "        entityCheck(full_table_pos, full_table_ner, index, hit_index, clutter_flag)\n",
    "\n",
    "    ## Updating the predicted TP results into one different column called \"Hit&Miss\"\n",
    "    for i in hit_index:\n",
    "        df.loc[i,'Hit&Miss'] = \"t\"\n",
    "\n",
    "    df[\"Hit&Miss\"].fillna(\"f\", inplace=True)\n",
    "\n",
    "    tp_precision, percentScore_tp = accuracyScore_TP()\n",
    "    print(str(percentScore_tp) + \" for true positives\")\n",
    "\n",
    "    fn_precision, percentScore_fn = accuracyScore_FN()\n",
    "    print(str(percentScore_fn) + \" for false negatives\")\n",
    "\n",
    "    fp_precision, percentScore_fp = accuracyScore_FP()\n",
    "    print(str(percentScore_fp) + \" for false positives\")\n",
    "\n",
    "    tn_precision, percentScore_tn = accuracyScore_TN()\n",
    "    print(str(percentScore_tn) + \" for true negatives\")\n",
    "\n",
    "    final_accuracy = (tp_precision + tn_precision)/(tp_precision + tn_precision + fp_precision + fn_precision)\n",
    "    print(\"Final accuracy seems like: \" + str(final_accuracy))\n",
    "\n",
    "    return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clutterRemoval(hit_index, ner_list):\n",
    "    \n",
    "    string1 = \"all rights reserved\"\n",
    "    string2 = \"distributed under the mit software license\"\n",
    "    string3 = df.loc[hit_index,'copyright']\n",
    "\n",
    "    if string1 in string3:\n",
    "        clutter_removed = string3[:string3.index(string1)]\n",
    "        df.loc[hit_index,'edited_text'] = clutter_removed \n",
    "\n",
    "    elif string2 in string3:\n",
    "        clutter_removed = string3[:string3.index(string1)]\n",
    "        df.iloc[hit_index,'edited_text'] = clutter_removed\n",
    "    \n",
    "    elif 'ORG' in ner_list['Values'] and 'PERSON' in ner_list['Values']:\n",
    "        org_name = ner_list['Entity'][ner_list['Values'].index('ORG')]\n",
    "        person_name = ner_list['Entity'][ner_list['Values'].index('PERSON')]\n",
    "        if string3.index(org_name) > string3.index(person_name):\n",
    "            clutter_removed = string3[:string3.index(org_name)] + org_name\n",
    "\n",
    "            df.iloc[hit_index,'edited_text'] = clutter_removed\n",
    "        else:    \n",
    "            clutter_removed = string3[:string3.index(person_name)] + person_name\n",
    "\n",
    "            df.iloc[hit_index,'edited_text'] = clutter_removed\n",
    "\n",
    "    \n",
    "    elif 'ORG' in ner_list['Values']:\n",
    "        org_name = ner_list['Entity'][ner_list['Values'].index('ORG')]\n",
    "\n",
    "        clutter_removed = string3[:string3.index(org_name)] + org_name\n",
    "        df.iloc[hit_index,'edited_text'] = clutter_removed\n",
    "\n",
    "    elif 'PERSON' in ner_list['Values']:\n",
    "        person_name = ner_list['Entity'][ner_list['Values'].index('PERSON')]\n",
    "        clutter_removed = string3[:string3.index(person_name)] + person_name\n",
    "        df.iloc[hit_index,'edited_text'] = clutter_removed\n",
    "    \n",
    "    else:\n",
    "        clutter_regex = r'''\"(Copyright\\s*(©)?([\\w \\-\\,\\[\\]]+)?(\\.com)?\\.?|Copyright\\s*(©)?|©([\\w \\-\\,\\.\\[\\]]+)?(Copyright)?)|(Copyright\\s*(\\(c\\))?|\\(c\\)\\s*(Copyright)?(?:[\\w \\,\\-\\.\\\"\\[\\]]{2,53}\\s*)?\\.?|\\(c\\)\\s*(Copyright)?)|Copyright|([\\w,]|(\\s*\\d+(\\s(?:\\,|-)\\s*\\d+)?\\s*))(\\s*\\d+(\\s*(?:\\,|-)\\s*\\d+)?\\,?\\s*)\\s*[a-zA-Z\\&\\| ,\\s0-9]{3,50}(\\.com)?\\.?|(\\.|\\,)?\\s*(\\@[^>]*?\\.com)|Inc+(\\.)?| Company+|Corporation+|& Co+|GmbH+|All rights reserved(?:\\.)?|Ltd|\\<|[\\w.]+@[a-zA-Z0-9-_.]+|\\>|([A-Za-z0-9]+\\.com)|rights\\s*reserved|(?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.)(?:\\([-\\w+&@#/%=~|$?!:,.]*\\)|[-\\w+&@#/%=~|$?!:,.])*(?:\\([\\w+&@#/%=~|$?!:,.]*\\)|[\\w+&@#/%=~|$])|\\<|\\>|\\(|\\)'''\n",
    "        clutter_removed = re.search(clutter_regex, string3)\n",
    "        df.loc[hit_index,'edited_text'] = clutter_removed\n",
    "\n",
    "  "
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}